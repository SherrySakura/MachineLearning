# 1. 概念
&emsp;&emsp;主成分分析（Pricipal Component Analysis，PCA）是最常用的一种降维方法，通过一个投影矩阵将可能存在相关性和冗余的特征转换为一组更低维度的线性不相关的特征，转换后的特征就叫做主成分。

# 2.  原理 
&emsp;&emsp;在降维的过程中，我们希望损失的信息尽可能少，也就是希望保留的信息尽可能多。PCA用方差来度量信息量，在某个维度上，样本分布越分散，方差越大，信息越多。因此，PCA对投影矩阵的第一个要求是使投影后的样本在各维度上方差尽可能大。

&emsp;&emsp;然而，如果单纯只选择方差最大的方向，会导致选择的基向量方向差不多，彼此相关性大，表示的信息几乎是重复的。所以为了使降维后的维度能尽可能地表达信息，第二个要求是不希望投影后的特征之间存在（线性）相关性。

&emsp;&emsp;综上，PCA的优化目标是：
 1. 降维后同一维度方差最大；
 2. 不同维度之间相关性为0。

&emsp;&emsp;根据线性代数可知，投影值的协方差矩阵的对角线代表了投影值在各个维度上的方差，其他元素代表各个维度之间的相关性（协方差）。基于优化目标，<font color="red" >**我们希望协方差矩阵是个对角线上值很大的对角矩阵（对角矩阵意味着非对角元素为0）**</font>。

&emsp;&emsp;假设数据样本已经进行了中心化，得到样本矩阵为$\mathbf {X}= \left( \boldsymbol { x } _ { 1 } , \boldsymbol { x } _ { 2 } , \ldots , \boldsymbol { x } _ { m } \right) \in \mathbb { R } ^ { d \times m }$，投影后样本矩阵为$Z = W ^ { T } X$，$\mathbf {W} \in \mathbb { R } ^ { d \times d ^ { \prime } }$是投影矩阵，将维度从$d$降到$d'$，$\mathbf { Z } \in \mathbb { R } ^ { d ^ { \prime } \times m }$是样本在新坐标系中的表达。**投影后**的协方差矩阵为：
$$
\begin{array} { l } { C = \frac { 1 } { m } Z Z^ { T } } \\ { = \frac { 1 } { m } \left( W ^ { T } X \right) \left( W ^ { T } X \right) ^ { T } } \\ { = \frac { 1 } { m } W ^ { T } X X ^ { T } W } \end{array}
$$

> &emsp;&emsp;在这里必须要声明的是，如果我们用$m$代表样本数，$d$代表维度数。
> 
> &emsp;&emsp;**1）如果$X$的维度如果是$m \times d$的话，$W$的维度依然是$d \times
> d'$，投影后样本矩阵为$Z = XW$，那么PCA要计算$X^TX$的协方差矩阵。**
> 
> &emsp;&emsp;2）**如果$X$的维度如果是$d \times m$的话，$W$的维度依然是$d \times
> d'$，投影后样本矩阵为$Z = W ^ { T } X$，那么PCA要计算$XX^T$的协方差矩阵。**
> 
> &emsp;&emsp;本文按照《机器学习（西瓜书）》里面的写法，采用<font
> color="red">**第二种表达方式**</font>，如果要进行代码实现，只需要把我们通常理解的样本矩阵进行转置变成新的$X$就可以了。

&emsp;&emsp;要使**投影后**的协方差矩阵为对角矩阵，就要找到能使**投影前**的协方差矩阵$XX ^ { T }$对角化的矩阵$W$（由于协方差矩阵$XX ^ { T }$是一个实对称矩阵，那么必然存在一个可逆矩阵可使其对角化，且相似对角阵上的元素即为矩阵本身特征值）。然后对**投影前**协方差矩阵$XX ^ { T }$进行特征值分解，将求得的特征值排序：$\lambda _ { 1 } \geq \lambda _ { 2 } \geq \ldots \geq \lambda _ { d }$，取前$d'$个特征值对应的特征向量构成$W  = \left( w _ { 1 } , w _ { 2 } , \dots , w _ { d ^ { \prime } } \right)$，这就是主成分分析的解。

&emsp;&emsp;严格来说，协方差矩阵是$\frac{1}{m}X{X^T}$，但前面常数项不影响，为方便描述我们指的的协方差矩阵是$X{X^T}$。

# 3. 严格推导过程
&emsp;&emsp;上一节中提到，PCA的优化目标为：

&emsp;&emsp;&emsp;&emsp;目标1：降维后同一维度方差最大；

&emsp;&emsp;&emsp;&emsp;目标2：不同维度之间相关性为0。
 
&emsp;&emsp;<font color="red">我们可以将PCA的优化目标转化为在不同维度之间相关性为0的约束条件下，求解使同一维度方差最大化的投影矩阵的问题。</font>

&emsp;&emsp;从两个角度，可以等价地解释方差最大化：

&emsp;&emsp;1. 最近重构性：样本点到投影的超平面（直线的高维推广）的距离都足够近；

&emsp;&emsp;2. 最大可分性：样本点在这个超平面上的投影都尽可能分开。

&emsp;&emsp;根据最近重构性和最大可分性，能够得到主成分分析的两种等价推导。

&emsp;&emsp;**（一）基于最近重构性推导PCA**

&emsp;&emsp;假定数据样本已经进行了中心化，即$\sum _ { i = 1 } ^ { m } x _ { i } = 0$（之所以进行中心化，是为了后面协方差表达式比较简洁）。投影前坐标系为$\left( x _ { 1 } , x _ { 2 } , \dots , x _ { d } \right)$，维度为$d$。如果丢弃部分坐标维度，将维度从$d$降低到$d'$，则<font color="red">标准正交基构成的投影矩阵$W$的维度为$d \times d'$</font>，投影变换后的新坐标系为$\left( w _ { 1 } , w _ { 2 } , \dots , w _ { d^ { \prime } } \right)$，其中$w_i$是标准正交基向量，$\left\| w _ { i } \right\| _ { 2 } = 1, w _ { i } ^ { T } w _ { j } = 0 ( i \neq j )$。样本点$x_i$在低维坐标系中的投影为$z _ { i } = \left( z _ { i 1 } , z _ { i 2 } , \dots , z _ { i d ^ { \prime } } \right)$，$z _ { i j } = w _ { j } ^ { T } x _ { i }$是$x_i$在低维坐标系下第$j$维的坐标。若基于$z_j$来重构$x_i$，则得到$\hat { x } _ { i } = \sum _ { j= 1 } ^ { d' } z _ { i j } w _ { j } = W z _ { i }$。

&emsp;&emsp;考虑整个训练集，我们想要使原样本点$x_i$与基于投影重构的样本点$\hat { x } _ { i }$之间的距离为：
$$
\begin{array}{l}
\sum\limits_{i = 1}^m {\left\| {{{\widehat x}_i} - {x_i}} \right\|_2^2}  = \sum\limits_{i = 1}^m {\left\| {\sum\limits_{j = 1}^{d'} {{z_{ij}}{w_j} - {x_i}} } \right\|} _2^2\\
 = \sum\limits_{i = 1}^m {{{\left( {W{z_i} - {x_i}} \right)}^2}} \\
 = \sum\limits_{i = 1}^m {\left[ {{{\left( {W{z_i}} \right)}^T}\left( {W{z_i}} \right) - 2{{\left( {W{z_i}} \right)}^T}{x_i} + x_i^T{x_i}} \right]} \\
 = \sum\limits_{i = 1}^m {\left( {z_i^T{W^T}W{z_i} - 2z_i^T{W^T}{x_i} + x_i^T{x_i}} \right)} \\
\mathop  = \limits^{{W^T}W = I,{z_i} = {W^T}{x_i}} \sum\limits_{i = 1}^m {\left( {z_i^T{z_i} - 2z_i^T{z_i} + x_i^T{x_i}} \right)} \\
 = \sum\limits_{i = 1}^m {\left( { - z_i^T{z_i} + x_i^T{x_i}} \right)} \\
\mathop  = \limits^{{z_i} = {W^T}{x_i}}  - \sum\limits_{i = 1}^m {{{\left( {{W^T}{x_i}} \right)}^T}\left( {{W^T}{x_i}} \right)}  + \sum\limits_{i = 1}^m {x_i^T{x_i}} \\
{\rm{ = }} - \sum\limits_{i = 1}^m {x_i^TW{W^T}x_i}  + \sum\limits_{i = 1}^m {x_i^T{x_i}} 
\end{array}\tag {1}
$$
&emsp;&emsp;有人可能会有疑问，该式中的$WW^T$不是等于$I$吗，那么式（1）不就等于0了吗？<font color="red">注意，$W$是$d \times d'$维度的矩阵，不是正交矩阵，尽管${W^T}W = I$，但$W{W^T} \ne I$，不能和式（1）的第二项抵消。</font>


&emsp;&emsp;由于${x_i^TW{W^T}x_i}$是一个标量，不是一个向量，可以用迹$tr\left( {x_i^TW{W^T}{x_i}} \right)$来代替，(1)式就变成了：
$$
\begin{array}{l}- tr\left( {\sum\limits_{i = 1}^m {x_i^TW{W^T}{x_i}} } \right) + \sum\limits_{i = 1}^m {x_i^T{x_i}} \\
\mathop  = \limits^{tr\left( {AB} \right) = tr\left( {BA} \right)}  - tr\left( {{W^T}\left( {\sum\limits_{i = 1}^m {{x_i}x_i^T} } \right)W} \right) + \sum\limits_{i = 1}^m {x_i^T{x_i}} \\
 =  - tr\left( {{W^T}X{X^T}W} \right) + \sum\limits_{i = 1}^m {x_i^T{x_i}} 
\end{array}
\tag {2}
$$

&emsp;&emsp;由于$x_i$的维度是$d \times 1$，$\sum\limits_{i = 1}^m {x_i^T{x_i}}$是一个常量，且$W$的每一个向量$w_j$是标准正交基，因此，最小化上式中的距离等价于：
$$
\underbrace { \arg \min } _ { W } - \operatorname { tr } \left( W ^ { T } X X ^ { T } W \right) \text { s.t. } W ^ { T } W = I\tag {3}
$$

&emsp;&emsp;**（二）基于最大可分性推导PCA**

&emsp;&emsp;样本点$x_i$在空间中超平面上的投影为$W^Tx_i$，如果要让所有样本点的投影尽可能分开，应使投影后的方差最大化，如图1所示。

<div align="center">
<img src="https://img-blog.csdnimg.cn/20181220131054868.png" width="40%"  alt=""/>
</div>
<div align="center">
图1：使所有样本的投影尽可能分开，则需最大化投影点的方差
</div>

&emsp;&emsp;由于投影值的协方差矩阵的对角线代表了投影值在各个维度上的方差，则所有维度上的方差和可写成协方差矩阵的迹：（这个过程与第2节中描述的一致）
$$
\begin{array}{l}
\sum\limits_{i = 1}^m {\left( {{W^T}{x_i}} \right){{\left( {{W^T}{x_i}} \right)}^T}}  = \sum\limits_{i = 1}^m {{W^T}{x_i}x_i^TW} \\
 = tr\left( {{W^T}X{X^T}W} \right)
\end{array}\tag {4}
$$

&emsp;&emsp;因此，最大化方差等价于：
$$
\underbrace { \arg \max } _ { W } \operatorname { tr } \left( W ^ { T } X X ^ { T } W \right) \text { s.t. } W ^ { T } W = I\tag {5}
$$

&emsp;&emsp;(5)式与(3)式等价。

# 4. PCA求解
&emsp;&emsp;对(3)式和(5)式中的优化目标，利用拉格朗日乘子法可得：
$$
J(W) =  -tr \left( {{W^T}X{X^T}W} \right) + \lambda \left( {{W^T}W - I} \right)\tag {6}
$$
&emsp;&emsp;对$X$求导，由于$\frac{{\partial tr\left( {A^TB} \right)}}{{\partial A}} = B$，可得：
$$
\frac{{\partial J(W)}}{{\partial W}} = -X{X^T}W + \lambda W\tag {7}
$$
令导数为0，得：
$$X{X^T}W =   \lambda W\tag {8}$$

&emsp;&emsp;根据线性代数中的特征值分解$Ax =   \lambda x$可知上式是一个类似的问题。于是，只需要对协方差矩阵$X{X^T}$进行特征值分解，将求得的特征值排序：$\lambda _ { 1 } \geq \lambda _ { 2 } \geq \ldots \geq \lambda _ { d }$，取前$d'$个特征值对应的特征向量构成$W = \left( w _ { 1 } , w _ { 2 } , \dots , w _ { d ^ { \prime } } \right)$，这就是主成分分析的解。（为什么）

&emsp;&emsp; 降维后低维空间的维数$d'$通常是事先指定的，还可以设置一个重构阈值，例如$t=95%$，然后选取使下式成立的最小$d'$值：
$$
\frac { \sum _ { i = 1 } ^ { d ^ { \prime } } \lambda _ { i } } { \sum _ { i = 1 } ^ { d } \lambda _ { i } } \geq t\tag {9}
$$

# 5. PCA算法描述
&emsp;&emsp; 输入：训练数据集$D= \left\{ x _ { 1 } , x _ { 2 } , \cdots , x _ { m } \right\}$，低维空间维数$d'$。

&emsp;&emsp; 过程：
&emsp;&emsp; &emsp;&emsp; （1）对所有样本进行中心化：
$$
x_i = x_i - \frac { 1 } { m } \sum _ { j = 1 } ^ { m } x _j 
$$

&emsp;&emsp; &emsp;&emsp; （2）计算样本的协方差矩阵$X{X^T}$；

&emsp;&emsp; &emsp;&emsp; （3）对协方差矩阵$X{X^T}$做特征值分解，求出其特征值及其对应的特征向量；

&emsp;&emsp; &emsp;&emsp; （4）取最大的$d'$个特征值对应的特征向量$\left( w _ { 1 } , w _ { 2 } , \dots , w _ { d ^ { \prime } } \right)$；

&emsp;&emsp; &emsp;&emsp; （5）对样本集$D$中的每一个样本$x_i$，转化为新样本$z_i=W^Tx_i$，得到输出样本集$D'= \left\{ z _ { 1 } , z _ { 2 } , \cdots , z_ { m } \right\}$。

&emsp;&emsp; 输出：投影矩阵$W  = \left( w _ { 1 } , w _ { 2 } , \dots , w _ { d ^ { \prime } } \right)$和降维后的样本集$D'= \left\{ z _ { 1 } , z _ { 2 } , \cdots , z_ { m } \right\}$。

# 6. PCA优缺点
&emsp;&emsp;PCA算法的主要优点：
 1. 舍弃一部分信息量少的特征， 使样本的采样密度增大；
 2. 最小的特征值所对应的特征向量往往与噪声有关，舍弃它们可以起到去噪的作用（在信号处理中任务信号具有较大方差，噪声拥有较小方差）；
 3. 计算方法简单，主要运算是特征值分解，易于实现。

&emsp;&emsp;主要缺点：
 1. 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强；
 2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。

> 参考文献：
> 1. 《机器学习》第十章降维与度量学习——周志华
> 2. [主成分分析（PCA）原理总结](https://www.cnblogs.com/pinard/p/6239403.html#!comments)
> 3. [协方差矩阵与PCA深入原理剖析](https://blog.csdn.net/u010309553/article/details/52497122)
> 4. [PCA算法原理：为什么用协方差矩阵](https://blog.csdn.net/a10767891/article/details/80288463)
> 5. [主成分分析（PCA）原理详解](https://blog.csdn.net/zhongkelee/article/details/44064401)
> 6. [图文并茂的PCA教程](https://blog.csdn.net/hustqb/article/details/78394058)
